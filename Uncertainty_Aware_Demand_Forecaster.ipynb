{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPTvqLkZNrJ1uro+sc+92iD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Farhana-Najnin/Uncertainty-Aware-Demand-Forecaster/blob/main/Uncertainty_Aware_Demand_Forecaster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4o2pD5La0uDM",
        "outputId": "85ef9d92-da72-4e3f-bed1-e15bfd79002b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Collecting datasets\n",
            "  Downloading datasets-4.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-1.3.3-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting pandas\n",
            "  Downloading pandas-3.0.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.4.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.10.8-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (52 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Collecting torch\n",
            "  Downloading torch-2.10.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (31 kB)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.53.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.5.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.3)\n",
            "Collecting pyarrow>=21.0.0 (from datasets)\n",
            "  Downloading pyarrow-23.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.2.0)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.5.4)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (0.21.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Collecting cuda-bindings==12.9.4 (from torch)\n",
            "  Downloading cuda_bindings-12.9.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (2.6 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Collecting nvidia-nvshmem-cu12==3.4.5 (from torch)\n",
            "  Downloading nvidia_nvshmem_cu12-3.4.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.6.0 (from torch)\n",
            "  Downloading triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch) (1.3.3)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=5.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.2.4)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.46)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.26.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.15.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.30.0)\n",
            "Downloading datasets-4.5.0-py3-none-any.whl (515 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m515.2/515.2 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-1.3.3-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.4.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.10.8-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.10.0-cp312-cp312-manylinux_2_28_x86_64.whl (915.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m915.7/915.7 MB\u001b[0m \u001b[31m692.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cuda_bindings-12.9.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m146.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvshmem_cu12-3.4.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (139.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.1/139.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (188.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.3/188.3 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading streamlit-1.53.1-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m144.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.5.0-py3-none-any.whl (24 kB)\n",
            "Downloading pyarrow-23.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m116.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, pyngrok, pyarrow, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, cuda-bindings, pydeck, pandas, nvidia-cusparse-cu12, nvidia-cufft-cu12, scikit-learn, nvidia-cusolver-cu12, matplotlib, huggingface_hub, torch, datasets, streamlit\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.5.0\n",
            "    Uninstalling triton-3.5.0:\n",
            "      Successfully uninstalled triton-3.5.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nvshmem-cu12\n",
            "    Found existing installation: nvidia-nvshmem-cu12 3.3.20\n",
            "    Uninstalling nvidia-nvshmem-cu12-3.3.20:\n",
            "      Successfully uninstalled nvidia-nvshmem-cu12-3.3.20\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufile-cu12\n",
            "    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n",
            "    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n",
            "      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: cuda-bindings\n",
            "    Found existing installation: cuda-bindings 12.9.5\n",
            "    Uninstalling cuda-bindings-12.9.5:\n",
            "      Successfully uninstalled cuda-bindings-12.9.5\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.36.0\n",
            "    Uninstalling huggingface-hub-0.36.0:\n",
            "      Successfully uninstalled huggingface-hub-0.36.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.9.0+cu126\n",
            "    Uninstalling torch-2.9.0+cu126:\n",
            "      Successfully uninstalled torch-2.9.0+cu126\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "fastai 2.8.6 requires torch<2.10,>=1.10, but you have torch 2.10.0 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.10.0 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "cuda-python 12.9.5 requires cuda-bindings~=12.9.5, but you have cuda-bindings 12.9.4 which is incompatible.\n",
            "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.10.0 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.1 which is incompatible.\n",
            "transformers 4.57.6 requires huggingface-hub<1.0,>=0.34.0, but you have huggingface-hub 1.3.3 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cuda-bindings-12.9.4 datasets-4.5.0 huggingface_hub-1.3.3 matplotlib-3.10.8 numpy-2.4.1 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.4.5 nvidia-nvtx-cu12-12.8.90 pandas-2.3.3 pyarrow-23.0.0 pydeck-0.9.1 pyngrok-7.5.0 scikit-learn-1.8.0 streamlit-1.53.1 torch-2.10.0 triton-3.6.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cuda",
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy"
                ]
              },
              "id": "98bba5707f6a423b8d1a9ffa22b48f7f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#BLOCK 1 â€” Install dependencies (Streamlit + Torch + Ngrok + HF datasets)\n",
        "!pip install -U datasets huggingface_hub pandas numpy scikit-learn matplotlib torch streamlit pyngrok\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After installing packages:\n",
        "\n",
        "ğŸ‘‰ Runtime â†’ Restart runtime\n"
      ],
      "metadata": {
        "id": "8zLXQT8X3i1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#BLOCK 3 â€” Import + ngrok setup\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Optional: paste your ngrok token (recommended to avoid limits)\n",
        "NGROK_TOKEN = \"38NyKf7cg5H2mBodKTcyO2bmvTA_4yNoV7ahmLv8VYGvEYCu5\"  # e.g. \"2xY.....\"\n",
        "\n",
        "if NGROK_TOKEN:\n",
        "    ngrok.set_auth_token(NGROK_TOKEN)\n",
        "\n",
        "print(\"ngrok ready\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quxItXkb02Z3",
        "outputId": "80d32387-e526-4878-e2a0-58d5db63d97e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrok ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BLOCK 4 â€” Write model.py\n",
        "%%writefile model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class GlobalLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Global multi-series LSTM with:\n",
        "    - series embedding\n",
        "    - dropout (used for MC Dropout uncertainty)\n",
        "    \"\"\"\n",
        "    def __init__(self, n_series: int, emb_dim: int = 16, hidden: int = 64, dropout: float = 0.25):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(n_series, emb_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.lstm = nn.LSTM(input_size=1 + emb_dim, hidden_size=hidden, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden, 1)\n",
        "\n",
        "    def forward(self, sid: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
        "        # sid: (B,)  x: (B, L)\n",
        "        B, L = x.shape\n",
        "        e = self.emb(sid)                      # (B, emb_dim)\n",
        "        e_seq = e.unsqueeze(1).repeat(1, L, 1) # (B, L, emb_dim)\n",
        "\n",
        "        x_seq = x.unsqueeze(-1)                # (B, L, 1)\n",
        "        inp = torch.cat([x_seq, e_seq], dim=-1)\n",
        "\n",
        "        out, _ = self.lstm(inp)\n",
        "        last = out[:, -1, :]\n",
        "        last = self.dropout(last)              # dropout active when model.train() during inference\n",
        "        yhat = self.fc(last).squeeze(-1)\n",
        "        return torch.clamp(yhat, min=0.0)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6TlpUjj1ZvE",
        "outputId": "3f5390bf-3182-491a-9842-adcf46f1fd03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BLOCK 5 â€” Write data_utils.py\n",
        "%%writefile data_utils.py\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "def load_store_sales_df() -> pd.DataFrame:\n",
        "    ds = load_dataset(\"t4tiana/store-sales-time-series-forecasting\")\n",
        "    df = ds[\"train\"].to_pandas()\n",
        "\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "    df = df.rename(columns={\"store_nbr\": \"store_id\", \"family\": \"item_id\", \"sales\": \"y\"})\n",
        "    df[\"series_id\"] = df[\"store_id\"].astype(str) + \"_\" + df[\"item_id\"].astype(str)\n",
        "\n",
        "    return df.sort_values([\"series_id\", \"date\"]).reset_index(drop=True)\n",
        "\n",
        "def train_val_split_last_h(df: pd.DataFrame, h: int = 28):\n",
        "    train_rows, val_rows = [], []\n",
        "    for sid, g in df.groupby(\"series_id\"):\n",
        "        if len(g) <= h:\n",
        "            continue\n",
        "        train_rows.append(g.iloc[:-h])\n",
        "        val_rows.append(g.iloc[-h:])\n",
        "    return pd.concat(train_rows).reset_index(drop=True), pd.concat(val_rows).reset_index(drop=True)\n",
        "\n",
        "def fit_global_scaler(train_df: pd.DataFrame):\n",
        "    y = train_df[\"y\"].astype(float).values\n",
        "    mu = float(np.mean(y))\n",
        "    sigma = float(np.std(y) + 1e-8)\n",
        "    return {\"mean\": mu, \"std\": sigma}\n",
        "\n",
        "def apply_scaler(df: pd.DataFrame, scaler: dict) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    out[\"y_scaled\"] = (out[\"y\"].astype(float) - scaler[\"mean\"]) / scaler[\"std\"]\n",
        "    return out\n",
        "\n",
        "def save_scaler(scaler: dict, path: str):\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump(scaler, f)\n",
        "\n",
        "def load_scaler(path: str) -> dict:\n",
        "    with open(path, \"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def build_series_index(df: pd.DataFrame):\n",
        "    series_list = sorted(df[\"series_id\"].unique().tolist())\n",
        "    sid2idx = {s:i for i,s in enumerate(series_list)}\n",
        "    idx2sid = {i:s for s,i in sid2idx.items()}\n",
        "    return sid2idx, idx2sid\n",
        "\n",
        "def make_windows(df_scaled: pd.DataFrame, sid2idx: dict, lookback: int = 56):\n",
        "    \"\"\"\n",
        "    Safe window builder:\n",
        "    - returns empty arrays if no windows exist (no crash)\n",
        "    \"\"\"\n",
        "    S, X, Y = [], [], []\n",
        "\n",
        "    for series_id, g in df_scaled.groupby(\"series_id\"):\n",
        "        g = g.sort_values(\"date\")\n",
        "        y = g[\"y_scaled\"].values.astype(np.float32)\n",
        "\n",
        "        if len(y) <= lookback:\n",
        "            continue\n",
        "\n",
        "        sid = sid2idx[series_id]\n",
        "        for i in range(lookback, len(y)):\n",
        "            S.append(sid)\n",
        "            X.append(y[i-lookback:i])\n",
        "            Y.append(y[i])\n",
        "\n",
        "    if len(X) == 0:\n",
        "        # Return empty but well-shaped arrays\n",
        "        return (\n",
        "            np.zeros((0,), dtype=np.int64),\n",
        "            np.zeros((0, lookback), dtype=np.float32),\n",
        "            np.zeros((0,), dtype=np.float32),\n",
        "        )\n",
        "\n",
        "    return (\n",
        "        np.array(S, dtype=np.int64),\n",
        "        np.stack(X).astype(np.float32),\n",
        "        np.array(Y, dtype=np.float32),\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cj0J0OG_1d1s",
        "outputId": "dceba1c8-e0a2-4dae-b75b-5ee650b14f49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing data_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BLOCK 6 â€” Write inventory.py\n",
        "\n",
        "%%writefile inventory.py\n",
        "import numpy as np\n",
        "\n",
        "def simulate_inventory_costs(\n",
        "    demand: np.ndarray,\n",
        "    forecast: np.ndarray,\n",
        "    lead_time: int = 7,\n",
        "    holding_cost: float = 1.0,\n",
        "    stockout_cost: float = 10.0\n",
        "):\n",
        "    demand = demand.astype(float)\n",
        "    forecast = forecast.astype(float)\n",
        "\n",
        "    inventory = 0.0\n",
        "    pipeline = [0.0] * lead_time\n",
        "\n",
        "    holding_total = 0.0\n",
        "    stockout_total = 0.0\n",
        "    stockout_events = 0\n",
        "\n",
        "    for d, f in zip(demand, forecast):\n",
        "        # receive arriving order\n",
        "        inventory += pipeline.pop(0)\n",
        "\n",
        "        # meet demand\n",
        "        if d > inventory:\n",
        "            unmet = d - inventory\n",
        "            stockout_total += unmet * stockout_cost\n",
        "            inventory = 0.0\n",
        "            stockout_events += 1\n",
        "        else:\n",
        "            inventory -= d\n",
        "\n",
        "        # holding cost (end-of-day)\n",
        "        holding_total += inventory * holding_cost\n",
        "\n",
        "        # place order to cover lead time using forecast\n",
        "        target = lead_time * max(f, 0.0)\n",
        "        inv_position = inventory + sum(pipeline)\n",
        "        order_qty = max(target - inv_position, 0.0)\n",
        "        pipeline.append(order_qty)\n",
        "\n",
        "    return {\n",
        "        \"total_cost\": holding_total + stockout_total,\n",
        "        \"holding_cost\": holding_total,\n",
        "        \"stockout_cost\": stockout_total,\n",
        "        \"stockout_events\": stockout_events,\n",
        "        \"days\": len(demand),\n",
        "    }\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zORtDUCG1d3q",
        "outputId": "822641bf-e4dd-4430-fc78-63e101fbb6ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing inventory.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BLOCK 7 â€” Write train_lstm.py (training + save artifacts)\n",
        "\n",
        "%%writefile train_lstm.py\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from model import GlobalLSTM\n",
        "from data_utils import (\n",
        "    load_store_sales_df,\n",
        "    train_val_split_last_h,\n",
        "    fit_global_scaler,\n",
        "    apply_scaler,\n",
        "    save_scaler,\n",
        "    build_series_index,\n",
        "    make_windows\n",
        ")\n",
        "\n",
        "class WindowDataset(Dataset):\n",
        "    def __init__(self, S, X, Y):\n",
        "        self.S = torch.tensor(S, dtype=torch.long)\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.Y = torch.tensor(Y, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.Y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.S[idx], self.X[idx], self.Y[idx]\n",
        "\n",
        "def main():\n",
        "    ART_DIR = \"artifacts\"\n",
        "    os.makedirs(ART_DIR, exist_ok=True)\n",
        "\n",
        "    LOOKBACK = 56\n",
        "    H = 28\n",
        "    MAX_SERIES = 250\n",
        "    EPOCHS = 3\n",
        "    BATCH = 512\n",
        "    LR = 1e-3\n",
        "\n",
        "    df = load_store_sales_df()\n",
        "\n",
        "    # Limit series for speed/stability in Colab\n",
        "    series_list = df[\"series_id\"].unique()[:MAX_SERIES]\n",
        "    df = df[df[\"series_id\"].isin(series_list)].copy()\n",
        "\n",
        "    train_df, _ = train_val_split_last_h(df, h=H)\n",
        "\n",
        "    # Scale\n",
        "    scaler = fit_global_scaler(train_df)\n",
        "    save_scaler(scaler, os.path.join(ART_DIR, \"scaler.json\"))\n",
        "    train_scaled = apply_scaler(train_df, scaler)\n",
        "\n",
        "    # Series index\n",
        "    sid2idx, _ = build_series_index(df)\n",
        "    n_series = len(sid2idx)\n",
        "\n",
        "    # Windows (TRAIN ONLY)\n",
        "    S_tr, X_tr, Y_tr = make_windows(train_scaled, sid2idx, lookback=LOOKBACK)\n",
        "\n",
        "    if len(Y_tr) == 0:\n",
        "        raise RuntimeError(\n",
        "            \"No training windows were created. \"\n",
        "            \"Try reducing LOOKBACK (e.g., 28) or increasing MAX_SERIES.\"\n",
        "        )\n",
        "\n",
        "    train_loader = DataLoader(WindowDataset(S_tr, X_tr, Y_tr), batch_size=BATCH, shuffle=True)\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = GlobalLSTM(n_series=n_series, dropout=0.25).to(device)\n",
        "\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "    loss_fn = torch.nn.L1Loss()\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total = 0.0\n",
        "        for sid, x, y in train_loader:\n",
        "            sid, x, y = sid.to(device), x.to(device), y.to(device)\n",
        "            opt.zero_grad()\n",
        "            pred = model(sid, x)\n",
        "            loss = loss_fn(pred, y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            total += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS} | train_loss={total/len(train_loader):.4f}\")\n",
        "\n",
        "    torch.save(\n",
        "        {\"state_dict\": model.state_dict(), \"n_series\": n_series, \"lookback\": LOOKBACK, \"max_series\": MAX_SERIES},\n",
        "        os.path.join(ART_DIR, \"lstm_mc_dropout.pt\")\n",
        "    )\n",
        "\n",
        "    print(\"Saved artifacts:\")\n",
        "    print(\"- artifacts/lstm_mc_dropout.pt\")\n",
        "    print(\"- artifacts/scaler.json\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNxxvCS51d7E",
        "outputId": "645924ef-9d90-4c23-b729-ccc510cf8398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train_lstm.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BLOCK 8 â€” Write app.py (Streamlit + MC Dropout + Inventory simulation)(AUTO-TRAIN FIX)\n",
        "%%writefile app.py\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "import torch\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "from model import GlobalLSTM\n",
        "from data_utils import load_store_sales_df, load_scaler, apply_scaler, build_series_index\n",
        "from inventory import simulate_inventory_costs\n",
        "\n",
        "st.set_page_config(page_title=\"Demand Forecasting Dashboard\", layout=\"wide\")\n",
        "\n",
        "# ---------- PATHS (absolute, Colab-safe) ----------\n",
        "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "ART_DIR = os.path.join(BASE_DIR, \"artifacts\")\n",
        "SCALER_PATH = os.path.join(ART_DIR, \"scaler.json\")\n",
        "MODEL_PATH = os.path.join(ART_DIR, \"lstm_mc_dropout.pt\")\n",
        "\n",
        "\n",
        "def ensure_artifacts_exist():\n",
        "    \"\"\"\n",
        "    If artifacts are missing, train the model ONCE from inside the app.\n",
        "    This prevents FileNotFoundError when opening public URL.\n",
        "    \"\"\"\n",
        "    os.makedirs(ART_DIR, exist_ok=True)\n",
        "\n",
        "    if os.path.exists(SCALER_PATH) and os.path.exists(MODEL_PATH):\n",
        "        return\n",
        "\n",
        "    st.warning(\"Model artifacts not found. Training now (one-time)...\")\n",
        "\n",
        "    # Run training script\n",
        "    result = subprocess.run(\n",
        "        [sys.executable, \"train_lstm.py\"],\n",
        "        capture_output=True,\n",
        "        text=True\n",
        "    )\n",
        "\n",
        "    # Show logs if something fails\n",
        "    if result.returncode != 0:\n",
        "        st.error(\"Training failed. Logs below:\")\n",
        "        st.code(result.stdout + \"\\n\" + result.stderr)\n",
        "        st.stop()\n",
        "\n",
        "    # Double check\n",
        "    if not (os.path.exists(SCALER_PATH) and os.path.exists(MODEL_PATH)):\n",
        "        st.error(\"Training completed but artifacts still missing. Check path issues.\")\n",
        "        st.stop()\n",
        "\n",
        "    st.success(\"Training complete. Artifacts created âœ…\")\n",
        "\n",
        "\n",
        "@st.cache_data\n",
        "def load_data(max_series=250):\n",
        "    df = load_store_sales_df()\n",
        "    series = df[\"series_id\"].unique()[:max_series]\n",
        "    df = df[df[\"series_id\"].isin(series)].copy()\n",
        "    df = df.sort_values([\"series_id\", \"date\"]).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "@st.cache_resource\n",
        "def load_artifacts():\n",
        "    ensure_artifacts_exist()\n",
        "\n",
        "    scaler = load_scaler(SCALER_PATH)\n",
        "    ckpt = torch.load(MODEL_PATH, map_location=\"cpu\")\n",
        "\n",
        "    model = GlobalLSTM(n_series=ckpt[\"n_series\"], dropout=0.25)\n",
        "    model.load_state_dict(ckpt[\"state_dict\"])\n",
        "    model.eval()\n",
        "\n",
        "    return scaler, ckpt, model\n",
        "\n",
        "\n",
        "def inverse_scale(arr_scaled, scaler):\n",
        "    return arr_scaled * scaler[\"std\"] + scaler[\"mean\"]\n",
        "\n",
        "\n",
        "def mc_forecast_quantiles(model, sid_idx, history_scaled, horizon=28, mc_samples=30, q_list=(50, 90)):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = model.to(device)\n",
        "\n",
        "    lookback = len(history_scaled)\n",
        "    x = history_scaled.astype(np.float32).copy()\n",
        "\n",
        "    preds_q = {q: [] for q in q_list}\n",
        "\n",
        "    for _ in range(horizon):\n",
        "        sid = torch.tensor([sid_idx], dtype=torch.long).to(device)\n",
        "        x_t = torch.tensor(x.reshape(1, lookback), dtype=torch.float32).to(device)\n",
        "\n",
        "        # enable dropout for MC sampling\n",
        "        model.train()\n",
        "\n",
        "        samples = []\n",
        "        with torch.no_grad():\n",
        "            for _m in range(mc_samples):\n",
        "                yhat = model(sid, x_t).cpu().numpy()[0]\n",
        "                samples.append(yhat)\n",
        "\n",
        "        samples = np.array(samples)\n",
        "        for q in q_list:\n",
        "            preds_q[q].append(np.percentile(samples, q))\n",
        "\n",
        "        # feed back P50\n",
        "        x = np.roll(x, -1)\n",
        "        x[-1] = preds_q[50][-1]\n",
        "\n",
        "    model.eval()\n",
        "    return {q: np.array(vals) for q, vals in preds_q.items()}\n",
        "\n",
        "\n",
        "# ---------- UI ----------\n",
        "st.title(\"Demand Forecasting (Global LSTM + MC Dropout) + Inventory Simulation\")\n",
        "\n",
        "with st.sidebar:\n",
        "    st.header(\"Controls\")\n",
        "    max_series = st.slider(\"Max series (speed)\", 50, 500, 250, step=50)\n",
        "    horizon = st.slider(\"Forecast horizon (days)\", 7, 60, 28, step=7)\n",
        "    lookback = st.slider(\"Lookback window (days)\", 28, 90, 56, step=7)\n",
        "    mc_samples = st.slider(\"MC Dropout samples\", 10, 100, 30, step=10)\n",
        "\n",
        "    st.subheader(\"Inventory policy\")\n",
        "    lead_time = st.slider(\"Lead time (days)\", 1, 21, 7, step=1)\n",
        "    holding_cost = st.number_input(\"Holding cost (per unit)\", value=1.0, min_value=0.0, step=0.5)\n",
        "    stockout_cost = st.number_input(\"Stockout cost (per unit unmet)\", value=10.0, min_value=0.0, step=1.0)\n",
        "    service_q = st.selectbox(\"Forecast used for ordering\", options=[50, 90], index=1)\n",
        "\n",
        "df = load_data(max_series=max_series)\n",
        "scaler, ckpt, model = load_artifacts()\n",
        "sid2idx, _ = build_series_index(df)\n",
        "\n",
        "series_choices = df[\"series_id\"].unique().tolist()\n",
        "series_id = st.selectbox(\"Choose a store-family series\", series_choices, index=0)\n",
        "\n",
        "g = df[df[\"series_id\"] == series_id].sort_values(\"date\").copy()\n",
        "if len(g) < lookback + horizon:\n",
        "    st.warning(\"Series too short for lookback+horizon. Pick another or reduce sliders.\")\n",
        "    st.stop()\n",
        "\n",
        "g_scaled = apply_scaler(g, scaler)\n",
        "\n",
        "history = g_scaled[\"y_scaled\"].values[-lookback:]\n",
        "actual_window = g[\"y\"].values[-horizon:]\n",
        "dates_window = g[\"date\"].values[-horizon:]\n",
        "\n",
        "sid_idx = sid2idx[series_id]\n",
        "\n",
        "qs = mc_forecast_quantiles(\n",
        "    model=model,\n",
        "    sid_idx=sid_idx,\n",
        "    history_scaled=history,\n",
        "    horizon=horizon,\n",
        "    mc_samples=mc_samples,\n",
        "    q_list=(50, 90),\n",
        ")\n",
        "\n",
        "p50 = inverse_scale(qs[50], scaler)\n",
        "p90 = inverse_scale(qs[90], scaler)\n",
        "\n",
        "order_forecast = p90 if service_q == 90 else p50\n",
        "\n",
        "inv_policy = simulate_inventory_costs(actual_window, order_forecast, lead_time, holding_cost, stockout_cost)\n",
        "inv_p50 = simulate_inventory_costs(actual_window, p50, lead_time, holding_cost, stockout_cost)\n",
        "inv_p90 = simulate_inventory_costs(actual_window, p90, lead_time, holding_cost, stockout_cost)\n",
        "\n",
        "c1, c2 = st.columns([2, 1])\n",
        "\n",
        "with c1:\n",
        "    st.subheader(\"Forecast Plot (P50 / P90)\")\n",
        "    plot_df = pd.DataFrame({\"date\": dates_window, \"actual\": actual_window, \"p50\": p50, \"p90\": p90}).set_index(\"date\")\n",
        "    st.line_chart(plot_df)\n",
        "\n",
        "with c2:\n",
        "    st.subheader(\"Inventory KPIs\")\n",
        "    st.write(f\"**Ordering policy:** P{service_q}\")\n",
        "    st.metric(\"Total Cost\", f\"{inv_policy['total_cost']:.2f}\")\n",
        "    st.metric(\"Holding Cost\", f\"{inv_policy['holding_cost']:.2f}\")\n",
        "    st.metric(\"Stockout Cost\", f\"{inv_policy['stockout_cost']:.2f}\")\n",
        "    st.metric(\"Stockout Events\", f\"{inv_policy['stockout_events']} / {inv_policy['days']}\")\n",
        "\n",
        "    st.subheader(\"Policy Comparison\")\n",
        "    st.write(\"P50:\", inv_p50)\n",
        "    st.write(\"P90:\", inv_p90)\n",
        "\n",
        "st.subheader(\"Recent Data\")\n",
        "st.dataframe(g[[\"date\", \"store_id\", \"item_id\", \"y\"]].tail(25), use_container_width=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7pWpF8A1vYw",
        "outputId": "e6d8a454-fac5-4e77-b730-193d338e7f19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BLOCK 9 â€” Train the model (creates artifacts/)\n",
        "!python train_lstm.py\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j06gkK3H1vao",
        "outputId": "3c1c4138-bcd1-4b6f-d286-894e99213ddb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rREADME.md: 0.00B [00:00, ?B/s]\rREADME.md: 3.24kB [00:00, 5.21MB/s]\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "train.csv: 100% 122M/122M [00:02<00:00, 60.5MB/s] \n",
            "test.csv: 1.02MB [00:00, 153MB/s]\n",
            "Generating train split: 3000888 examples [00:02, 1015509.21 examples/s]\n",
            "Generating test split: 28512 examples [00:00, 968465.00 examples/s]\n",
            "Epoch 1/3 | train_loss=0.3231\n",
            "Epoch 2/3 | train_loss=0.3049\n",
            "Epoch 3/3 | train_loss=0.3013\n",
            "Saved artifacts:\n",
            "- artifacts/lstm_mc_dropout.pt\n",
            "- artifacts/scaler.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BLOCK 10 â€” Run Streamlit + expose with ngrok (public URL)\n",
        "import subprocess,time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "try: ngrok.kill()\n",
        "except: pass\n",
        "\n",
        "subprocess.Popen([\n",
        " \"streamlit\",\"run\",\"app.py\",\n",
        " \"--server.port=8501\",\n",
        " \"--server.address=0.0.0.0\",\n",
        " \"--server.enableCORS=false\",\n",
        " \"--server.enableXsrfProtection=false\"\n",
        "])\n",
        "\n",
        "time.sleep(3)\n",
        "print(\"PUBLIC URL:\",ngrok.connect(8501,\"http\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI_1KTAg1veP",
        "outputId": "bdd69775-750f-45a6-e552-325a5b34755a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PUBLIC URL: NgrokTunnel: \"https://endurable-thwartedly-somer.ngrok-free.dev\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}